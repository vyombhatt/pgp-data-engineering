{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gPsk3Uu3H0dS",
    "outputId": "65ecaebf-c16a-462d-f89c-2b2ac0d007fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=514393cbb82d7cc1c4b54a9f90c43b963173fb1ff88c7d34723f9d3812854e00\n",
      "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
      "Successfully built pyspark\n",
      "Installing collected packages: pyspark\n",
      "Successfully installed pyspark-3.5.1\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hc-hINbhHwPZ",
    "outputId": "a49361c1-e0ad-4afe-9199-5f315442f812"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/02 17:23:06 WARN Utils: Your hostname, Jyants-MacBook-Air-2.local resolves to a loopback address: 127.0.0.1; using 10.103.162.22 instead (on interface en0)\n",
      "24/06/02 17:23:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/02 17:23:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/06/02 17:23:07 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.5.1\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Introduction to Spark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Check the Spark version\n",
    "print(\"Spark Version:\", spark.version)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oZDnaEd1H-51",
    "outputId": "c75672fa-0c90-4203-cf02-998ccfb2e999"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/02 17:23:08 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Alice| 34|\n",
      "|  Bob| 45|\n",
      "|Cathy| 29|\n",
      "+-----+---+\n",
      "\n",
      "+-----+\n",
      "| Name|\n",
      "+-----+\n",
      "|Alice|\n",
      "|  Bob|\n",
      "|Cathy|\n",
      "+-----+\n",
      "\n",
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Alice| 34|\n",
      "|  Bob| 45|\n",
      "+-----+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|Age|count|\n",
      "+---+-----+\n",
      "| 34|    1|\n",
      "| 45|    1|\n",
      "| 29|    1|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataFrame Operations\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create a DataFrame from a list of tuples\n",
    "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Cathy\", 29)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n",
    "# Select a single column\n",
    "df.select(\"Name\").show()\n",
    "\n",
    "# Filter rows\n",
    "df.filter(df.Age > 30).show()\n",
    "\n",
    "# Group by and aggregate\n",
    "df.groupBy(\"Age\").count().show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "g6DZqN9qIvVx"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    {\"name\": \"Alice\", \"age\": 34, \"city\": \"New York\"},\n",
    "    {\"name\": \"Bob\", \"age\": 45, \"city\": \"San Francisco\"},\n",
    "    {\"name\": \"Cathy\", \"age\": 29, \"city\": \"Los Angeles\"}\n",
    "]\n",
    "\n",
    "# Write the sample data to a JSON file with each record in a new line\n",
    "with open('sample_data.json', 'w') as f:\n",
    "    for entry in data:\n",
    "        json.dump(entry, f)\n",
    "        f.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zeW_s-JDIwEo",
    "outputId": "0afca235-f90c-4cb0-ab7f-b9509bb8f0be"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/02 17:23:20 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----+\n",
      "|age|         city| name|\n",
      "+---+-------------+-----+\n",
      "| 34|     New York|Alice|\n",
      "| 45|San Francisco|  Bob|\n",
      "| 29|  Los Angeles|Cathy|\n",
      "+---+-------------+-----+\n",
      "\n",
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+---+-------------+-----+\n",
      "|age|         city| name|\n",
      "+---+-------------+-----+\n",
      "| 34|     New York|Alice|\n",
      "| 45|San Francisco|  Bob|\n",
      "+---+-------------+-----+\n",
      "\n",
      "+-----+\n",
      "| name|\n",
      "+-----+\n",
      "|Alice|\n",
      "|  Bob|\n",
      "|Cathy|\n",
      "+-----+\n",
      "\n",
      "+---+-------------+-----+\n",
      "|age|         city| name|\n",
      "+---+-------------+-----+\n",
      "| 34|     New York|Alice|\n",
      "| 45|San Francisco|  Bob|\n",
      "+---+-------------+-----+\n",
      "\n",
      "+-------------+-----+\n",
      "|         city|count|\n",
      "+-------------+-----+\n",
      "|  Los Angeles|    1|\n",
      "|San Francisco|    1|\n",
      "|     New York|    1|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark SQL and DataFrames with Sample Data\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the sample JSON file into a DataFrame\n",
    "df = spark.read.json(\"sample_data.json\")\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n",
    "# Display the schema of the DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# Perform SQL operations on the DataFrame\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "result = spark.sql(\"SELECT * FROM people WHERE age > 30\")\n",
    "result.show()\n",
    "\n",
    "# Additional DataFrame Operations\n",
    "\n",
    "# Select a single column\n",
    "df.select(\"name\").show()\n",
    "\n",
    "# Filter rows\n",
    "df.filter(df.age > 30).show()\n",
    "\n",
    "# Group by and aggregate\n",
    "df.groupBy(\"city\").count().show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "geK_ZNoeIxtN",
    "outputId": "8b332447-b0e8-4275-f016-04a0c6006542"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/02 17:23:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---+------+\n",
      "|first_name|last_name|age|gender|\n",
      "+----------+---------+---+------+\n",
      "|      John|      Doe| 30|     M|\n",
      "|      Jane|      Doe| 25|     F|\n",
      "|       Sam|    Smith| 35|     M|\n",
      "|     Sally|    Brown| 28|     F|\n",
      "+----------+---------+---+------+\n",
      "\n",
      "root\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n",
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|      John|      Doe|\n",
      "|      Jane|      Doe|\n",
      "|       Sam|    Smith|\n",
      "|     Sally|    Brown|\n",
      "+----------+---------+\n",
      "\n",
      "+----------+---------+---+------+\n",
      "|first_name|last_name|age|gender|\n",
      "+----------+---------+---+------+\n",
      "|       Sam|    Smith| 35|     M|\n",
      "+----------+---------+---+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|gender|count|\n",
      "+------+-----+\n",
      "|     M|    2|\n",
      "|     F|    2|\n",
      "+------+-----+\n",
      "\n",
      "+----------+---------+---+------+\n",
      "|first_name|last_name|age|gender|\n",
      "+----------+---------+---+------+\n",
      "|      Jane|      Doe| 25|     F|\n",
      "|     Sally|    Brown| 28|     F|\n",
      "+----------+---------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark SQL and DataFrames - Example 1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [(\"John\", \"Doe\", 30, \"M\"),\n",
    "        (\"Jane\", \"Doe\", 25, \"F\"),\n",
    "        (\"Sam\", \"Smith\", 35, \"M\"),\n",
    "        (\"Sally\", \"Brown\", 28, \"F\")]\n",
    "\n",
    "# Define schema\n",
    "columns = [\"first_name\", \"last_name\", \"age\", \"gender\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n",
    "# Display the schema of the DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# Select specific columns\n",
    "df.select(\"first_name\", \"last_name\").show()\n",
    "\n",
    "# Filter rows based on a condition\n",
    "df.filter(df.age > 30).show()\n",
    "\n",
    "# Group by a column and count\n",
    "df.groupBy(\"gender\").count().show()\n",
    "\n",
    "# Create a temporary view and run SQL queries\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "result = spark.sql(\"SELECT * FROM people WHERE age < 30\")\n",
    "result.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W-X5oS6vJOt9",
    "outputId": "456fcb86-1525-4dea-fa0b-d339835bcab4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/02 17:23:28 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+\n",
      "| name|dept_id|salary|\n",
      "+-----+-------+------+\n",
      "| John|      1|  1000|\n",
      "| Jane|      2|  1500|\n",
      "|  Sam|      1|  2000|\n",
      "|Sally|      2|  2500|\n",
      "+-----+-------+------+\n",
      "\n",
      "+-------+---------+\n",
      "|dept_id|dept_name|\n",
      "+-------+---------+\n",
      "|      1|       HR|\n",
      "|      2|  Finance|\n",
      "+-------+---------+\n",
      "\n",
      "+-------+-----+------+---------+\n",
      "|dept_id| name|salary|dept_name|\n",
      "+-------+-----+------+---------+\n",
      "|      1| John|  1000|       HR|\n",
      "|      1|  Sam|  2000|       HR|\n",
      "|      2| Jane|  1500|  Finance|\n",
      "|      2|Sally|  2500|  Finance|\n",
      "+-------+-----+------+---------+\n",
      "\n",
      "+---------+----------+------------+\n",
      "|dept_name|avg_salary|total_salary|\n",
      "+---------+----------+------------+\n",
      "|       HR|    1500.0|        3000|\n",
      "|  Finance|    2000.0|        4000|\n",
      "+---------+----------+------------+\n",
      "\n",
      "+---------+----------+\n",
      "|dept_name|avg_salary|\n",
      "+---------+----------+\n",
      "|       HR|    1500.0|\n",
      "|  Finance|    2000.0|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, sum\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark SQL and DataFrames - Example 2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample data for employees\n",
    "employees = [(\"John\", 1, 1000),\n",
    "             (\"Jane\", 2, 1500),\n",
    "             (\"Sam\", 1, 2000),\n",
    "             (\"Sally\", 2, 2500)]\n",
    "\n",
    "# Sample data for departments\n",
    "departments = [(1, \"HR\"),\n",
    "               (2, \"Finance\")]\n",
    "\n",
    "# Define schemas\n",
    "emp_columns = [\"name\", \"dept_id\", \"salary\"]\n",
    "dept_columns = [\"dept_id\", \"dept_name\"]\n",
    "\n",
    "# Create DataFrames\n",
    "emp_df = spark.createDataFrame(employees, schema=emp_columns)\n",
    "dept_df = spark.createDataFrame(departments, schema=dept_columns)\n",
    "\n",
    "# Show the DataFrames\n",
    "emp_df.show()\n",
    "dept_df.show()\n",
    "\n",
    "# Join the DataFrames\n",
    "joined_df = emp_df.join(dept_df, on=\"dept_id\", how=\"inner\")\n",
    "joined_df.show()\n",
    "\n",
    "# Perform aggregations\n",
    "joined_df.groupBy(\"dept_name\").agg(\n",
    "    avg(\"salary\").alias(\"avg_salary\"),\n",
    "    sum(\"salary\").alias(\"total_salary\")\n",
    ").show()\n",
    "\n",
    "# Create a temporary view and run SQL queries\n",
    "joined_df.createOrReplaceTempView(\"employee_dept\")\n",
    "result = spark.sql(\"SELECT dept_name, AVG(salary) AS avg_salary FROM employee_dept GROUP BY dept_name\")\n",
    "result.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "oKf8mMrgJRTl"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'sc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m\n\u001b[1;32m      2\u001b[0m data \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      3\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlice\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m29\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcity\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNew York\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      4\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBob\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m31\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcity\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSan Francisco\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      5\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCathy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m25\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcity\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLos Angeles\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      6\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDavid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m35\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcity\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChicago\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m      7\u001b[0m ]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Create a DataFrame from the JSON data\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Show the DataFrame\u001b[39;00m\n\u001b[1;32m     13\u001b[0m df\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pyspark/sql/session.py:1443\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m   1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m   1441\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m   1442\u001b[0m     )\n\u001b[0;32m-> 1443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1445\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pyspark/sql/session.py:1485\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1483\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1485\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pyspark/sql/session.py:1116\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;66;03m# convert python objects to sql data\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m internal_data \u001b[38;5;241m=\u001b[39m [struct\u001b[38;5;241m.\u001b[39mtoInternal(row) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m tupled_data]\n\u001b[0;32m-> 1116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallelize\u001b[49m\u001b[43m(\u001b[49m\u001b[43minternal_data\u001b[49m\u001b[43m)\u001b[49m, struct\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pyspark/context.py:783\u001b[0m, in \u001b[0;36mSparkContext.parallelize\u001b[0;34m(self, c, numSlices)\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallelize\u001b[39m(\u001b[38;5;28mself\u001b[39m, c: Iterable[T], numSlices: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RDD[T]:\n\u001b[1;32m    752\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    753\u001b[0m \u001b[38;5;124;03m    Distribute a local Python collection to form an RDD. Using range\u001b[39;00m\n\u001b[1;32m    754\u001b[0m \u001b[38;5;124;03m    is recommended if the input represents a range for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;124;03m    [['a'], ['b', 'c']]\u001b[39;00m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 783\u001b[0m     numSlices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(numSlices) \u001b[38;5;28;01mif\u001b[39;00m numSlices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefaultParallelism\u001b[49m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(c, \u001b[38;5;28mrange\u001b[39m):\n\u001b[1;32m    785\u001b[0m         size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(c)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pyspark/context.py:630\u001b[0m, in \u001b[0;36mSparkContext.defaultParallelism\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefaultParallelism\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    620\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;124;03m    Default level of parallelism to use when not given by user (e.g. for reduce tasks)\u001b[39;00m\n\u001b[1;32m    622\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;124;03m    True\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m()\u001b[38;5;241m.\u001b[39mdefaultParallelism()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'sc'"
     ]
    }
   ],
   "source": [
    "# Sample JSON data\n",
    "data = [\n",
    "    {\"name\": \"Alice\", \"age\": 29, \"city\": \"New York\"},\n",
    "    {\"name\": \"Bob\", \"age\": 31, \"city\": \"San Francisco\"},\n",
    "    {\"name\": \"Cathy\", \"age\": 25, \"city\": \"Los Angeles\"},\n",
    "    {\"name\": \"David\", \"age\": 35, \"city\": \"Chicago\"}\n",
    "]\n",
    "\n",
    "# Create a DataFrame from the JSON data\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
