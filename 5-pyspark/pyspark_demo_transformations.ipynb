{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "3ikeDugMNglG",
    "outputId": "2411754b-f67e-4856-bf48-ce42adb5b4a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"11.0.4\" 2019-07-16\n",
      "OpenJDK Runtime Environment (build 11.0.4+11-post-Ubuntu-1ubuntu218.04.3)\n",
      "OpenJDK 64-Bit Server VM (build 11.0.4+11-post-Ubuntu-1ubuntu218.04.3, mixed mode, sharing)\n"
     ]
    }
   ],
   "source": [
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "dkeH2p37NRGX",
    "outputId": "3a63b50c-daf2-4d4e-e461-f0c9f27b4ec6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 choices for the alternative java (providing /usr/bin/java).\n",
      "\n",
      "  Selection    Path                                            Priority   Status\n",
      "------------------------------------------------------------\n",
      "* 0            /usr/lib/jvm/java-11-openjdk-amd64/bin/java      1111      auto mode\n",
      "  1            /usr/lib/jvm/java-11-openjdk-amd64/bin/java      1111      manual mode\n",
      "  2            /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java   1081      manual mode\n",
      "\n",
      "Press <enter> to keep the current choice[*], or type selection number: 2\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java to provide /usr/bin/java (java) in manual mode\n"
     ]
    }
   ],
   "source": [
    "!sudo update-alternatives --config java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3hJ2KzB9M6pd"
   },
   "outputs": [],
   "source": [
    "!wget -q http://apachemirror.wuchna.com/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QuVi65UOoQS_"
   },
   "outputs": [],
   "source": [
    "!tar xf spark-2.4.4-bin-hadoop2.7.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D8HVHvTcoR6u"
   },
   "outputs": [],
   "source": [
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OHU5W-LhoS4W"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p8xtPzbCoS_W"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/08 21:47:55 WARN Utils: Your hostname, Jyants-MacBook-Air-2.local resolves to a loopback address: 127.0.0.1; using 10.103.162.22 instead (on interface en0)\n",
      "24/06/08 21:47:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/08 21:47:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/06/08 21:47:57 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ApmBKFUSoaTN"
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "colab_type": "code",
    "id": "eAMreSN4obWB",
    "outputId": "3d36eb95-0d73-4e69-f2e9-71a9eeca70a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/21/f05c186f4ddb01d15d0ddc36ef4b7e3cedbeb6412274a41f26b55a650ee5/pyspark-2.4.4.tar.gz (215.7MB)\n",
      "\u001b[K     |████████████████████████████████| 215.7MB 121kB/s \n",
      "\u001b[?25hCollecting py4j==0.10.7 (from pyspark)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
      "\u001b[K     |████████████████████████████████| 204kB 53.2MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-2.4.4-py2.py3-none-any.whl size=216130387 sha256=112526630226fedf1839233729199bb6d0376f0292e3e8e649a59c9249f7319f\n",
      "  Stored in directory: /root/.cache/pip/wheels/ab/09/4d/0d184230058e654eb1b04467dbc1292f00eaa186544604b471\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.7 pyspark-2.4.4\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "py4j",
         "pyspark"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oia_0gHqDXjQ"
   },
   "source": [
    "***User Defined Functions***\n",
    "**Scalar Pandas UDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "GMwu4M40DijS",
    "outputId": "db5fd58a-8695-4924-8c16-4a547976e02a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/08 21:51:36 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "/Users/jyantmahara/anaconda3/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:485: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if should_localize and is_datetime64tz_dtype(s.dtype) and s.dt.tz is not None:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    4\n",
      "2    9\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|multiply_func(x, x)|\n",
      "+-------------------+\n",
      "|                  1|\n",
      "|                  4|\n",
      "|                  9|\n",
      "+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jyantmahara/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").appName(\" UDF Example\").config(\"spark.some.config.option\", \"some-value\").getOrCreate()\n",
    "\n",
    "# Declare the function and create the UDF\n",
    "def multiply_func(a, b):\n",
    "    return a * b\n",
    "\n",
    "multiply = pandas_udf(multiply_func, returnType=LongType())\n",
    "\n",
    "# The function for a pandas_udf should be able to execute with local Pandas data\n",
    "x = pd.Series([1, 2, 3])\n",
    "print(multiply_func(x, x))\n",
    "# 0    1\n",
    "# 1    4\n",
    "# 2    9\n",
    "# dtype: int64\n",
    "\n",
    "# Create a Spark DataFrame, 'spark' is an existing SparkSession\n",
    "df = spark.createDataFrame(pd.DataFrame(x, columns=[\"x\"]))\n",
    "\n",
    "# Execute function as a Spark vectorized UDF\n",
    "df.select(multiply(col(\"x\"), col(\"x\"))).show()\n",
    "# +-------------------+\n",
    "# |multiply_func(x, x)|\n",
    "# +-------------------+\n",
    "# |                  1|\n",
    "# |                  4|\n",
    "# |                  9|\n",
    "# +-------------------+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DzO83gcGGQVg"
   },
   "source": [
    "***User Defined Functions***\n",
    "**Group By**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "sAC9QQsoGV-l",
    "outputId": "36955ac9-4b20-491b-f96a-babe74c50674"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jyantmahara/anaconda3/lib/python3.10/site-packages/pyspark/sql/pandas/group_ops.py:104: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|   v|\n",
      "+---+----+\n",
      "|  1|-0.5|\n",
      "|  1| 0.5|\n",
      "|  2|-3.0|\n",
      "|  2|-1.0|\n",
      "|  2| 4.0|\n",
      "+---+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jyantmahara/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "/Users/jyantmahara/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "\r",
      "[Stage 3:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
    "    (\"id\", \"v\"))\n",
    "\n",
    "@pandas_udf(\"id long, v double\", PandasUDFType.GROUPED_MAP)\n",
    "def substract_mean(pdf):\n",
    "    # pdf is a pandas.DataFrame\n",
    "    v = pdf.v\n",
    "    return pdf.assign(v=v - v.mean())\n",
    "\n",
    "df.groupby(\"id\").apply(substract_mean).show()\n",
    "# +---+----+\n",
    "# | id|   v|\n",
    "# +---+----+\n",
    "# |  1|-0.5|\n",
    "# |  1| 0.5|\n",
    "# |  2|-3.0|\n",
    "# |  2|-1.0|\n",
    "# |  2| 4.0|\n",
    "# +---+----+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "q_8xhAnNvW55",
    "outputId": "688d4a77-f49e-445f-b250-80988bf83871"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession.Builder at 0x7f80f27f0208>"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "SparkSession.builder.config(conf=SparkConf())\n",
    "SparkSession.builder.config(\"spark.some.config.option\", \"some-value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i3PhJeTfoeHm"
   },
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uj0_LO_kofUG"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "import pandas as pd\n",
    "conf = SparkConf()\n",
    "conf.setMaster('local')\n",
    "conf.setAppName('spark-basic')\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MBp-HQ_3vuHC",
    "outputId": "b3c92dee-a9ec-4077-8cb3-1cbf8a92b817"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = SparkSession.builder.config(\"k1\", \"v1\").getOrCreate()\n",
    "s1.conf.get(\"k1\") == s1.sparkContext.getConf().get(\"k1\") == \"v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HGw3WNFrwJu1"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local\").appName(\"Word Count\").config(\"spark.some.config.option\", \"some-value\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "uT0T-k8xv1r6",
    "outputId": "54ce7b51-755a-4e25-9441-f560deeca2d0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2 = SparkSession.builder.config(\"k2\", \"v2\").getOrCreate()\n",
    "s1.conf.get(\"k1\") == s2.conf.get(\"k1\")\n",
    "s1.conf.get(\"k2\") == s2.conf.get(\"k2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P4a4OY-tGyKX"
   },
   "source": [
    "**Making sample dataframe from Titanic Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f2Jn7641qqeM"
   },
   "outputs": [],
   "source": [
    "data1 = {'PassengerId': {0: 1, 1: 2, 2: 3, 3: 4, 4: 5},\n",
    "         'Name': {0: 'Owen', 1: 'Florence', 2: 'Laina', 3: 'Lily', 4: 'William'},\n",
    "         'Sex': {0: 'male', 1: 'female', 2: 'female', 3: 'female', 4: 'male'},\n",
    "         'Survived': {0: 0, 1: 1, 2: 1, 3: 1, 4: 0}}\n",
    "\n",
    "data2 = {'PassengerId': {0: 1, 1: 2, 2: 3, 3: 4, 4: 5},\n",
    "         'Age': {0: 22, 1: 38, 2: 26, 3: 35, 4: 35},\n",
    "         'Fare': {0: 7.3, 1: 71.3, 2: 7.9, 3: 53.1, 4: 8.0},\n",
    "         'Pclass': {0: 3, 1: 1, 2: 3, 3: 1, 4: 3}}\n",
    "\n",
    "df1_pd = pd.DataFrame(data1, columns=data1.keys())\n",
    "df2_pd = pd.DataFrame(data2, columns=data2.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tIHAoRzgG4MV"
   },
   "source": [
    "**Printing Pandas Dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "5gvlwW8PqtE8",
    "outputId": "e0ab8327-b120-4d08-d5df-5e48aa7f4337"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Owen</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Florence</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Lily</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>William</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId      Name     Sex  Survived\n",
       "0            1      Owen    male         0\n",
       "1            2  Florence  female         1\n",
       "2            3     Laina  female         1\n",
       "3            4      Lily  female         1\n",
       "4            5   William    male         0"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_5Jhx1S7vo1f",
    "outputId": "25e391ee-e2ec-4e66-8335-b0addcc8f7b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1='Alice', _2=1)]"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [('Alice', 1)]\n",
    "spark.createDataFrame(l).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "40f8c_5JkUu5"
   },
   "source": [
    "**Converting Pandas DF to Spark DF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "txmWv6Ggq7hc",
    "outputId": "1fdcda9a-3325-45ae-eda0-3ad3dbbf1a74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+\n",
      "|PassengerId|    Name|   Sex|Survived|\n",
      "+-----------+--------+------+--------+\n",
      "|          1|    Owen|  male|       0|\n",
      "|          2|Florence|female|       1|\n",
      "|          3|   Laina|female|       1|\n",
      "|          4|    Lily|female|       1|\n",
      "|          5| William|  male|       0|\n",
      "+-----------+--------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame(df1_pd)\n",
    "df2 = spark.createDataFrame(df2_pd)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0O_U--ODmEag"
   },
   "source": [
    "**Transformation - Select**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "kcbJ5rpMw-kq",
    "outputId": "52f27f40-5163-43dc-8e22-83588399a04b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|PassengerId|    Name|\n",
      "+-----------+--------+\n",
      "|          1|    Owen|\n",
      "|          2|Florence|\n",
      "|          3|   Laina|\n",
      "|          4|    Lily|\n",
      "|          5| William|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols1 = ['PassengerId', 'Name']\n",
    "df1.select(cols1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5WHPrY6VmJHh"
   },
   "source": [
    "**Transformation - Filter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "PMslaFV8mEFi",
    "outputId": "b1cb88b8-ba95-44cd-936e-4f9a63876719"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+\n",
      "|PassengerId|    Name|   Sex|Survived|\n",
      "+-----------+--------+------+--------+\n",
      "|          2|Florence|female|       1|\n",
      "|          3|   Laina|female|       1|\n",
      "|          4|    Lily|female|       1|\n",
      "+-----------+--------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.filter(df1.Sex == 'female').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yl0Z-_pzmP8J"
   },
   "source": [
    "**Transformation - Mutate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "FdGMqCNKmU7x",
    "outputId": "68d86094-961f-46e7-ebb1-77ac17962a23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+----+------+------------+\n",
      "|PassengerId|Age|Fare|Pclass|AgeTimesFare|\n",
      "+-----------+---+----+------+------------+\n",
      "|          1| 22| 7.3|     3|       160.6|\n",
      "|          2| 38|71.3|     1|      2709.4|\n",
      "|          3| 26| 7.9|     3|       205.4|\n",
      "|          4| 35|53.1|     1|      1858.5|\n",
      "|          5| 35| 8.0|     3|       280.0|\n",
      "+-----------+---+----+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.withColumn('AgeTimesFare', df2.Age*df2.Fare).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z9Pm3wOxmeaH"
   },
   "source": [
    "**Transformation - Summarize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "llW9AtcjmeoK",
    "outputId": "42246764-13ca-4e60-e4de-b9597bd6232b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x7f80f08594a8>"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf2 = df2.groupby('Pclass')\n",
    "gdf2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_eEz60a7mltw"
   },
   "source": [
    "**Average of Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "W92p5GHAmnw2",
    "outputId": "3d65de26-6957-4447-e2e5-c6e070f979e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+-----------------+\n",
      "|Pclass|          avg(Age)|        avg(Fare)|\n",
      "+------+------------------+-----------------+\n",
      "|     1|              36.5|             62.2|\n",
      "|     3|27.666666666666668|7.733333333333333|\n",
      "+------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_cols = ['Age', 'Fare']\n",
    "gdf2.avg(*avg_cols).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hDBGc9bLmsfv"
   },
   "source": [
    "**Passing a dctionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "D-CUVzmUmudZ",
    "outputId": "7fb1406b-52fd-451f-a77d-35e8687cff1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------------------+---------+\n",
      "|Pclass|count(1)|          avg(Age)|sum(Fare)|\n",
      "+------+--------+------------------+---------+\n",
      "|     1|       2|              36.5|    124.4|\n",
      "|     3|       3|27.666666666666668|     23.2|\n",
      "+------+--------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gdf2.agg({'*': 'count', 'Age': 'avg', 'Fare':'sum'}).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nbUDgzKImuTX"
   },
   "source": [
    "**Renaming columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "CMA7weepm2Z-",
    "outputId": "bef16e3d-7874-46ce-a5a8-4c6104249ceb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------------------+----------+\n",
      "|Pclass|counts|       average_age|total_fare|\n",
      "+------+------+------------------+----------+\n",
      "|     1|     2|              36.5|     124.4|\n",
      "|     3|     3|27.666666666666668|      23.2|\n",
      "+------+------+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    gdf2\n",
    "    .agg({'*': 'count', 'Age': 'avg', 'Fare':'sum'})\n",
    "    .toDF('Pclass', 'counts', 'average_age', 'total_fare')\n",
    "    .show()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c77HwyWLnLOQ"
   },
   "source": [
    "**Transformation - Arrange**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "ReKMwsKInQdR",
    "outputId": "df7e9eb8-f39d-4351-fa70-56b176729b35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+----+------+\n",
      "|PassengerId|Age|Fare|Pclass|\n",
      "+-----------+---+----+------+\n",
      "|          2| 38|71.3|     1|\n",
      "|          4| 35|53.1|     1|\n",
      "|          5| 35| 8.0|     3|\n",
      "|          3| 26| 7.9|     3|\n",
      "|          1| 22| 7.3|     3|\n",
      "+-----------+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.sort('Fare', ascending=False).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TXDwx9E4nOGA"
   },
   "source": [
    "**Transformation - Joins**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "_y1hPETSnVnX",
    "outputId": "610f14ef-eb34-408e-d50c-708ffad52f95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+---+----+------+\n",
      "|PassengerId|    Name|   Sex|Survived|Age|Fare|Pclass|\n",
      "+-----------+--------+------+--------+---+----+------+\n",
      "|          5| William|  male|       0| 35| 8.0|     3|\n",
      "|          1|    Owen|  male|       0| 22| 7.3|     3|\n",
      "|          3|   Laina|female|       1| 26| 7.9|     3|\n",
      "|          2|Florence|female|       1| 38|71.3|     1|\n",
      "|          4|    Lily|female|       1| 35|53.1|     1|\n",
      "+-----------+--------+------+--------+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2, ['PassengerId']).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "67_DmkyNnQLJ"
   },
   "source": [
    "**Transformation - Nonequi joins**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "aG9W2oIunclI",
    "outputId": "e9ffbf8d-847d-4614-90a7-8685c881347f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+-----------+---+----+------+\n",
      "|PassengerId|    Name|   Sex|Survived|PassengerId|Age|Fare|Pclass|\n",
      "+-----------+--------+------+--------+-----------+---+----+------+\n",
      "|          1|    Owen|  male|       0|          1| 22| 7.3|     3|\n",
      "|          1|    Owen|  male|       0|          2| 38|71.3|     1|\n",
      "|          1|    Owen|  male|       0|          3| 26| 7.9|     3|\n",
      "|          1|    Owen|  male|       0|          4| 35|53.1|     1|\n",
      "|          1|    Owen|  male|       0|          5| 35| 8.0|     3|\n",
      "|          2|Florence|female|       1|          2| 38|71.3|     1|\n",
      "|          2|Florence|female|       1|          3| 26| 7.9|     3|\n",
      "|          2|Florence|female|       1|          4| 35|53.1|     1|\n",
      "|          2|Florence|female|       1|          5| 35| 8.0|     3|\n",
      "|          3|   Laina|female|       1|          3| 26| 7.9|     3|\n",
      "|          3|   Laina|female|       1|          4| 35|53.1|     1|\n",
      "|          3|   Laina|female|       1|          5| 35| 8.0|     3|\n",
      "|          4|    Lily|female|       1|          4| 35|53.1|     1|\n",
      "|          4|    Lily|female|       1|          5| 35| 8.0|     3|\n",
      "|          5| William|  male|       0|          5| 35| 8.0|     3|\n",
      "+-----------+--------+------+--------+-----------+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2, df1.PassengerId <= df2.PassengerId).show() \n",
    "# Note the duplicate col names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZDY6jc5eniCV"
   },
   "source": [
    "**Transformation - Unions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "QnjW915FnlfY",
    "outputId": "cb616bb7-175e-4c41-e290-58a2a329f35b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+\n",
      "|PassengerId|    Name|   Sex|Survived|\n",
      "+-----------+--------+------+--------+\n",
      "|          1|    Owen|  male|       0|\n",
      "|          2|Florence|female|       1|\n",
      "|          3|   Laina|female|       1|\n",
      "|          4|    Lily|female|       1|\n",
      "|          5| William|  male|       0|\n",
      "|          1|    Owen|  male|       0|\n",
      "|          2|Florence|female|       1|\n",
      "|          3|   Laina|female|       1|\n",
      "|          4|    Lily|female|       1|\n",
      "|          5| William|  male|       0|\n",
      "+-----------+--------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.union(df1).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rixAdP4s2dv8"
   },
   "source": [
    "**Inferring Schema**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I-Xx-rX94xBO"
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "f = urllib.request.urlretrieve (\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\", \"kddcup.data_10_percent.gz\")\n",
    "\n",
    "\n",
    "data_file = \"./kddcup.data_10_percent.gz\"\n",
    "raw_data = sc.textFile(data_file).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f7e11At_49tL"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eelYoD1n2dgD"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "csv_data = raw_data.map(lambda l: l.split(\",\"))\n",
    "row_data = csv_data.map(lambda p: Row(\n",
    "    duration=int(p[0]), \n",
    "    protocol_type=p[1],\n",
    "    service=p[2],\n",
    "    flag=p[3],\n",
    "    src_bytes=int(p[4]),\n",
    "    dst_bytes=int(p[5])\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1yCO87RB45ct"
   },
   "outputs": [],
   "source": [
    "interactions_df = sqlContext.createDataFrame(row_data)\n",
    "interactions_df.registerTempTable(\"interactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "colab_type": "code",
    "id": "b-HQCq1n5GB_",
    "outputId": "f37deaad-d9a2-4314-d309-d8615c1ec910"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|duration|dst_bytes|\n",
      "+--------+---------+\n",
      "|    5057|        0|\n",
      "|    5059|        0|\n",
      "|    5051|        0|\n",
      "|    5056|        0|\n",
      "|    5051|        0|\n",
      "|    5039|        0|\n",
      "|    5062|        0|\n",
      "|    5041|        0|\n",
      "|    5056|        0|\n",
      "|    5064|        0|\n",
      "|    5043|        0|\n",
      "|    5061|        0|\n",
      "|    5049|        0|\n",
      "|    5061|        0|\n",
      "|    5048|        0|\n",
      "|    5047|        0|\n",
      "|    5044|        0|\n",
      "|    5063|        0|\n",
      "|    5068|        0|\n",
      "|    5062|        0|\n",
      "+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select tcp network interactions with more than 1 second duration and no transfer from destination\n",
    "tcp_interactions = sqlContext.sql(\"\"\"\n",
    "    SELECT duration, dst_bytes FROM interactions WHERE protocol_type = 'tcp' AND duration > 1000 AND dst_bytes = 0\n",
    "\"\"\")\n",
    "tcp_interactions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ym3QRJ72MbbO"
   },
   "source": [
    "**Parquet Files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "OBm-peJiMd_x",
    "outputId": "7b2e61a0-50b3-4360-849a-b27ae2816fd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  name|\n",
      "+------+\n",
      "|Justin|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDF = spark.read.json(\"people.json\")\n",
    "\n",
    "# DataFrames can be saved as Parquet files, maintaining the schema information.\n",
    "peopleDF.write.parquet(\"people.parquet\")\n",
    "\n",
    "# Read in the Parquet file created above.\n",
    "# Parquet files are self-describing so the schema is preserved.\n",
    "# The result of loading a parquet file is also a DataFrame.\n",
    "parquetFile = spark.read.parquet(\"people.parquet\")\n",
    "\n",
    "# Parquet files can also be used to create a temporary view and then used in SQL statements.\n",
    "parquetFile.createOrReplaceTempView(\"parquetFile\")\n",
    "teenagers = spark.sql(\"SELECT name FROM parquetFile WHERE age >= 13 AND age <= 19\")\n",
    "teenagers.show()\n",
    "# +------+\n",
    "# |  name|\n",
    "# +------+\n",
    "# |Justin|\n",
    "# +------+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JxtKUkj87VGm"
   },
   "source": [
    "**JSON File**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "gzB_DVrB7XX9",
    "outputId": "40bdbf47-d4c2-4e63-f53b-18ea0640f7b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+------+\n",
      "|  name|\n",
      "+------+\n",
      "|Justin|\n",
      "+------+\n",
      "\n",
      "+----------------+----+\n",
      "|         address|name|\n",
      "+----------------+----+\n",
      "|[Columbus, Ohio]| Yin|\n",
      "+----------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "\n",
    "# A JSON dataset is pointed to by path.\n",
    "# The path can be either a single text file or a directory storing text files\n",
    "path = \"people.json\"\n",
    "peopleDF = spark.read.json(path)\n",
    "\n",
    "# The inferred schema can be visualized using the printSchema() method\n",
    "peopleDF.printSchema()\n",
    "# root\n",
    "#  |-- age: long (nullable = true)\n",
    "#  |-- name: string (nullable = true)\n",
    "\n",
    "# Creates a temporary view using the DataFrame\n",
    "peopleDF.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# SQL statements can be run by using the sql methods provided by spark\n",
    "teenagerNamesDF = spark.sql(\"SELECT name FROM people WHERE age BETWEEN 13 AND 19\")\n",
    "teenagerNamesDF.show()\n",
    "# +------+\n",
    "# |  name|\n",
    "# +------+\n",
    "# |Justin|\n",
    "# +------+\n",
    "\n",
    "# Alternatively, a DataFrame can be created for a JSON dataset represented by\n",
    "# an RDD[String] storing one JSON object per string\n",
    "jsonStrings = ['{\"name\":\"Yin\",\"address\":{\"city\":\"Columbus\",\"state\":\"Ohio\"}}']\n",
    "otherPeopleRDD = sc.parallelize(jsonStrings)\n",
    "otherPeople = spark.read.json(otherPeopleRDD)\n",
    "otherPeople.show()\n",
    "# +---------------+----+\n",
    "# |        address|name|\n",
    "# +---------------+----+\n",
    "# |[Columbus,Ohio]| Yin|\n",
    "# +---------------+----+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5x9TEuZC53VH"
   },
   "source": [
    "**SPARK WITH HIVE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EuXD74joPyn0"
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vYHsKKc357s-"
   },
   "source": [
    "**Import Statements**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kmDC_Lxb55sc"
   },
   "outputs": [],
   "source": [
    "from os.path import expanduser, join, abspath\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/09 12:44:23 WARN Utils: Your hostname, Jyants-MacBook-Air-2.local resolves to a loopback address: 127.0.0.1; using 10.103.162.22 instead (on interface en0)\n",
      "24/06/09 12:44:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/09 12:44:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"MySparkApp\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7N2FgFU06CSH"
   },
   "source": [
    "**Warehouse Location**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fSzYYnKb6Eso"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/09 12:44:27 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "warehouse_location = abspath('spark-warehouse')\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Python Spark SQL Hive integration example\").config(\"spark.sql.warehouse.dir\", warehouse_location).enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RLll_lN16Hpj"
   },
   "source": [
    "**Spark in existing SparkSession**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LUaKK8uHcQAA",
    "outputId": "e02027c7-d080-4037-a3bc-f0da4b07d763"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/09 12:44:45 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive\")\n",
    "spark.sql(\"LOAD DATA LOCAL INPATH 'kv1.txt' INTO TABLE src\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wP07ncmUQBkk"
   },
   "source": [
    "**Queries are expressed in Hive QL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "colab_type": "code",
    "id": "I2Hro5Ge8vtY",
    "outputId": "70c930fd-9f56-4a89-c53a-3aa11bc81282"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/09 12:44:50 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| key|value|\n",
      "+----+-----+\n",
      "|NULL| NULL|\n",
      "|NULL| NULL|\n",
      "|NULL| NULL|\n",
      "|NULL| NULL|\n",
      "|NULL| NULL|\n",
      "|NULL| NULL|\n",
      "|NULL| NULL|\n",
      "|NULL| NULL|\n",
      "|NULL| NULL|\n",
      "|NULL| NULL|\n",
      "|NULL| NULL|\n",
      "|NULL| NULL|\n",
      "|NULL| NULL|\n",
      "|NULL| NULL|\n",
      "|NULL| NULL|\n",
      "|NULL| NULL|\n",
      "|NULL| NULL|\n",
      "|NULL| NULL|\n",
      "|NULL| NULL|\n",
      "|NULL| NULL|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM src\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XjobCi1v6uuu"
   },
   "source": [
    "**Aggregation Queries also supported**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "Kd6dXxXvQMKc",
    "outputId": "2b0795e4-7656-4edd-88a8-f192ca5a47a4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/09 12:44:56 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "24/06/09 12:44:56 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/06/09 12:44:56 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|      21|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT COUNT(*) FROM src\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nPAVVdlAQPvM"
   },
   "source": [
    "**Result of SQL Queries = Dataframes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-iMA-xSbQUfY"
   },
   "outputs": [],
   "source": [
    "sqlDF = spark.sql(\"SELECT key, value FROM src WHERE key < 10 ORDER BY key\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4mh3IOYSQUNU"
   },
   "source": [
    "**Each column can be accessed in order**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "vLeTCGE5QaGs",
    "outputId": "d189752c-2c80-4a8a-e244-720ed80bf00f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: 0, Value: val_0\n",
      "Key: 0, Value: val_0\n",
      "Key: 0, Value: val_0\n",
      "Key: 2, Value: val_2\n",
      "Key: 4, Value: val_4\n",
      "Key: 5, Value: val_5\n",
      "Key: 5, Value: val_5\n",
      "Key: 5, Value: val_5\n",
      "Key: 8, Value: val_8\n",
      "Key: 9, Value: val_9\n"
     ]
    }
   ],
   "source": [
    "stringsDS = sqlDF.rdd.map(lambda row: \"Key: %d, Value: %s\" % (row.key, row.value))\n",
    "for record in stringsDS.collect():\n",
    "    print(record)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eZeQAxMrRmnz"
   },
   "source": [
    "**Temporary views withing a SparkSession**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D8yVXzhdRqmO"
   },
   "outputs": [],
   "source": [
    "Record = Row(\"key\", \"value\")\n",
    "recordsDF = spark.createDataFrame([Record(i, \"val_\" + str(i)) for i in range(1, 101)])\n",
    "recordsDF.createOrReplaceTempView(\"records\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wlswnBKwRqbD"
   },
   "source": [
    "**Dataframes and Hive data joined**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "colab_type": "code",
    "id": "7gNh-jbxRy7F",
    "outputId": "562b7a9b-295e-4959-f73f-96be1c5dad3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------+\n",
      "|key| value|key| value|\n",
      "+---+------+---+------+\n",
      "|  2| val_2|  2| val_2|\n",
      "|  4| val_4|  4| val_4|\n",
      "|  5| val_5|  5| val_5|\n",
      "|  5| val_5|  5| val_5|\n",
      "|  5| val_5|  5| val_5|\n",
      "|  8| val_8|  8| val_8|\n",
      "|  9| val_9|  9| val_9|\n",
      "| 10|val_10| 10|val_10|\n",
      "| 11|val_11| 11|val_11|\n",
      "| 12|val_12| 12|val_12|\n",
      "| 12|val_12| 12|val_12|\n",
      "| 15|val_15| 15|val_15|\n",
      "| 15|val_15| 15|val_15|\n",
      "| 17|val_17| 17|val_17|\n",
      "| 18|val_18| 18|val_18|\n",
      "| 18|val_18| 18|val_18|\n",
      "| 19|val_19| 19|val_19|\n",
      "| 20|val_20| 20|val_20|\n",
      "| 24|val_24| 24|val_24|\n",
      "| 24|val_24| 24|val_24|\n",
      "+---+------+---+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM records r JOIN src s ON r.key = s.key\").show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "PySpark Module 5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
