{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce287798",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Initialize the SparkContext\n",
    "sc = SparkContext(\"local[*]\", \"PySparkStreamingExample\")\n",
    "\n",
    "# Initialize the StreamingContext with a 1-second batch duration\n",
    "ssc = StreamingContext(sc, 1)\n",
    "\n",
    "# Define the hostname and port to listen to\n",
    "hostname = \"localhost\"\n",
    "port = 9999\n",
    "\n",
    "# Create a DStream that will connect to hostname:port\n",
    "lines = ssc.socketTextStream(hostname, port)\n",
    "\n",
    "# Split each line into words\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# Count each word in each batch\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Print the first ten elements of each RDD generated in this DStream to the console\n",
    "wordCounts.pprint()\n",
    "\n",
    "# Start the computation\n",
    "ssc.start()\n",
    "\n",
    "# Wait for the computation to terminate\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ada75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Initialize the SparkContext\n",
    "sc = SparkContext(\"local[*]\", \"LogMonitoringExample\")\n",
    "\n",
    "# Initialize the StreamingContext with a 1-second batch duration\n",
    "ssc = StreamingContext(sc, 1)\n",
    "\n",
    "# Define the hostname and port to listen to\n",
    "hostname = \"localhost\"\n",
    "port = 9999\n",
    "\n",
    "# Create a DStream that will connect to hostname:port\n",
    "lines = ssc.socketTextStream(hostname, port)\n",
    "\n",
    "# Filter the lines to only include error messages\n",
    "errors = lines.filter(lambda line: \"ERROR\" in line)\n",
    "\n",
    "# Count each error message in each batch\n",
    "errorCounts = errors.map(lambda error: (error, 1)).reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Print the first ten elements of each RDD generated in this DStream to the console\n",
    "errorCounts.pprint()\n",
    "\n",
    "# Start the computation\n",
    "ssc.start()\n",
    "\n",
    "# Wait for the computation to terminate\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507244a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "import json\n",
    "\n",
    "# Function to extract text and metadata from raw tweets\n",
    "def extract_text_and_metadata(raw_tweet):\n",
    "    tweet = json.loads(raw_tweet)\n",
    "    return tweet['text'], tweet['language']\n",
    "\n",
    "# Function to check if the tweet is in English\n",
    "def is_english(tweet):\n",
    "    return tweet[1] == 'en'\n",
    "\n",
    "# Function to analyze sentiment (dummy function for illustration)\n",
    "def analyze_sentiment(tweet):\n",
    "    text = tweet[0]\n",
    "    if \"love\" in text or \"great\" in text:\n",
    "        return 1\n",
    "    elif \"ERROR\" in text or \"wrong\" in text:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Initialize the SparkContext\n",
    "sc = SparkContext(\"local[*]\", \"SentimentAnalysis\")\n",
    "\n",
    "# Initialize the StreamingContext with a 1-second batch duration\n",
    "ssc = StreamingContext(sc, 1)\n",
    "\n",
    "# Define the hostname and port to listen to\n",
    "hostname = \"localhost\"\n",
    "port = 9999\n",
    "\n",
    "# Create a DStream that will connect to hostname:port\n",
    "lines = ssc.socketTextStream(hostname, port)\n",
    "\n",
    "# Extract text and metadata from raw tweets\n",
    "tweets = lines.map(lambda raw_tweet: extract_text_and_metadata(raw_tweet))\n",
    "\n",
    "# Filter out non-English tweets\n",
    "english_tweets = tweets.filter(lambda tweet: is_english(tweet))\n",
    "\n",
    "# Perform sentiment analysis and get sentiment scores\n",
    "sentiment_scores = english_tweets.map(lambda tweet: analyze_sentiment(tweet))\n",
    "\n",
    "# Aggregate sentiment scores in each batch\n",
    "total_sentiment = sentiment_scores.reduce(lambda x, y: x + y)\n",
    "\n",
    "# Print the aggregated sentiment score\n",
    "total_sentiment.pprint()\n",
    "\n",
    "# Start the computation\n",
    "ssc.start()\n",
    "\n",
    "# Wait for the computation to terminate\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00db7549",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accumulators and Broadcast Variables in Apache Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6639752b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(\"local\", \"Accumulator Example\")\n",
    "accum = sc.accumulator(0)\n",
    "\n",
    "def process_line(line):\n",
    "    global accum\n",
    "    if \"ERROR\" in line:\n",
    "        accum += 1\n",
    "    return line\n",
    "\n",
    "log_rdd = sc.textFile(\"logs.txt\")\n",
    "log_rdd.foreach(process_line)\n",
    "\n",
    "print(\"Total Errors:\", accum.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f11573",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304d57a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(\"local\", \"Broadcast Example\")\n",
    "lookup_data = {\"a\": 1, \"b\": 2, \"c\": 3}\n",
    "broadcast_var = sc.broadcast(lookup_data)\n",
    "\n",
    "def transform(line):\n",
    "    key = line.split(\",\")[0]\n",
    "    return broadcast_var.value.get(key, 0)\n",
    "\n",
    "rdd = sc.parallelize([\"a,alpha\", \"b,beta\", \"c,gamma\"])\n",
    "result = rdd.map(transform).collect()\n",
    "\n",
    "print(result)  # Output: [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cb94bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba30f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Function to update the state with new values\n",
    "def updateFunction(new_values, running_count):\n",
    "    return sum(new_values) + (running_count or 0)\n",
    "\n",
    "# Initialize the SparkContext\n",
    "sc = SparkContext(\"local[*]\", \"StatefulWordCount\")\n",
    "\n",
    "# Initialize the StreamingContext with a 1-second batch duration\n",
    "ssc = StreamingContext(sc, 1)\n",
    "\n",
    "# Set the checkpoint directory\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "# Define the hostname and port to listen to\n",
    "hostname = \"localhost\"\n",
    "port = 9999\n",
    "\n",
    "# Create a DStream that will connect to hostname:port\n",
    "lines = ssc.socketTextStream(hostname, port)\n",
    "\n",
    "# Split each line into words\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# Map each word to a pair (word, 1)\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "\n",
    "# Update the cumulative count using updateStateByKey\n",
    "statefulWordCounts = pairs.updateStateByKey(updateFunction)\n",
    "\n",
    "# Print the first ten elements of each RDD generated in this DStream to the console\n",
    "statefulWordCounts.pprint()\n",
    "\n",
    "# Start the computation\n",
    "ssc.start()\n",
    "\n",
    "# Wait for the computation to terminate\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90644547",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b35ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1042d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import StreamingClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d133f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import OAuthHandler\n",
    "from tweepy.streaming import StreamingClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97293b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "BEARER_TOKEN = '%2FI%3DSLqJDBSzn6umIpnCI7SWGw7NPefRlHaZSplEfCFiHyRsz6'\n",
    "url = \"https://api.twitter.com/2/tweets/search/stream/rules\"\n",
    "headers = {\"Authorization\": f\"Bearer {BEARER_TOKEN}\"}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01826c7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
